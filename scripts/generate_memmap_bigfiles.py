#!/usr/bin/env python
"""ä½¿ç”¨å†…å­˜æ˜ å°„çš„å¿«é€Ÿå¤§æ–‡ä»¶ç”Ÿæˆå™¨ - æ— éœ€h5py"""

import argparse
import json
from pathlib import Path
import os
import sys
import numpy as np
import torch
import torch.distributed as dist
import time
from tqdm import tqdm
from datasets import load_dataset
import gc
from typing import List, Optional

os.environ["TOKENIZERS_PARALLELISM"] = "false"

def setup_distributed():
    """Setup distributed training environment."""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', rank))
        
        # æ£€æŸ¥å¯ç”¨GPU
        available_gpus = torch.cuda.device_count()
        if local_rank >= available_gpus:
            print(f"Warning: local_rank {local_rank} >= available GPUs {available_gpus}")
            local_rank = local_rank % available_gpus
        
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend='nccl', init_method='env://')
        
        print(f"Rank {rank}/{world_size}, local_rank {local_rank}, device cuda:{local_rank}")
        return rank, world_size, local_rank
    else:
        return 0, 1, 0

class MemmapBigFileGenerator:
    """ä½¿ç”¨å†…å­˜æ˜ å°„çš„é«˜æ•ˆå¤§æ–‡ä»¶ç”Ÿæˆå™¨"""
    
    def __init__(self, args, rank, world_size, local_rank):
        self.args = args
        self.rank = rank
        self.world_size = world_size
        self.local_rank = local_rank
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(args.output_dir)
        self.base_name = f"train_rank{rank}"
        
        if rank == 0:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if world_size > 1:
            dist.barrier()
        
        # åŠ è½½æ•°æ®é›†
        print(f"Rank {rank}: Loading dataset...")
        self.dataset = self._load_dataset()
        
        # åˆ†é…æ•°æ®ç»™å„ä¸ªrank
        total_docs = len(self.dataset)
        docs_per_rank = total_docs // world_size
        self.start_idx = rank * docs_per_rank
        self.end_idx = self.start_idx + docs_per_rank if rank < world_size - 1 else total_docs
        
        # åŠ è½½æ¨¡å‹
        print(f"Rank {rank}: Loading model...")
        self.model, self.tokenizer = self._load_model_and_tokenizer()
        
        # åˆå§‹åŒ–å†…å­˜æ˜ å°„æ–‡ä»¶
        self._init_memmap_files()
        
        if rank == 0:
            print(f"\n{'='*60}")
            print(f"Memory-Mapped Big File Generator")
            print(f"{'='*60}")
            print(f"Output: {self.output_dir}/{self.base_name}_*.npy")
            print(f"Model: {args.model}")
            print(f"Model path: {args.model_path}")
            print(f"Embedding dim: {args.embedding_dim}")
            print(f"Max length: {args.max_length}")
            print(f"Batch size: {args.batch_size}")
            print(f"Max chars: {args.max_chars}")
            print(f"World size: {world_size}")
            print(f"Total docs: {total_docs:,}")
            print(f"Docs per rank: {self.end_idx - self.start_idx:,}")
            print(f"{'='*60}")
    
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        source_path = Path(self.args.source_dir) if self.args.source_dir else None
        
        if source_path and source_path.exists():
            print(f"Loading dataset from: {source_path}")
            
            # å°è¯•åŠ è½½ä¸ºdatasetsæ ¼å¼
            try:
                # æ‰¾parquetæ–‡ä»¶
                parquet_files = list(source_path.glob("*.parquet"))
                if parquet_files:
                    print(f"Found {len(parquet_files)} parquet files")
                    dataset = load_dataset("parquet", 
                                         data_files=[str(f) for f in parquet_files],
                                         split="train",
                                         num_proc=4)
                    print(f"Parquet dataset loaded: {len(dataset)} samples")
                    return dataset
                
                # æ‰¾arrowæ–‡ä»¶
                arrow_files = list(source_path.glob("*.arrow"))
                if arrow_files:
                    print(f"Found {len(arrow_files)} arrow files")
                    dataset = load_dataset("arrow", 
                                         data_files=[str(f) for f in arrow_files],
                                         split="train",
                                         num_proc=4)
                    print(f"Arrow dataset loaded: {len(dataset)} samples")
                    return dataset
                
                print("No supported dataset files found in source directory")
                
            except Exception as e:
                print(f"Error loading dataset from {source_path}: {e}")
        
        # å¦‚æœæŒ‡å®šäº†æ•°æ®é›†åç§°ï¼Œå°è¯•ä»huggingfaceä¸‹è½½
        if self.args.dataset:
            try:
                print(f"Loading dataset from HuggingFace: {self.args.dataset}")
                dataset = load_dataset(self.args.dataset, 
                                     split="train",
                                     num_proc=4,
                                     streaming=False)
                print(f"Loaded {len(dataset)} samples from {self.args.dataset}")
                return dataset
            except Exception as e:
                print(f"Error loading HuggingFace dataset: {e}")
        
        # é»˜è®¤åˆ›å»ºæµ‹è¯•æ•°æ®
        print("Creating sample data for testing...")
        from datasets import Dataset
        sample_texts = [{"text": f"This is sample text {i} for testing." * 10} 
                       for i in range(1000)]
        return Dataset.from_list(sample_texts)
    
    def _load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        model_path = self.args.model_path
        
        if not os.path.exists(model_path):
            raise ValueError(f"Model path does not exist: {model_path}")
        
        try:
            from sentence_transformers import SentenceTransformer
            from transformers import AutoTokenizer
            
            print(f"Loading model from: {model_path}")
            
            # åŠ è½½SentenceTransformeræ¨¡å‹
            model = SentenceTransformer(
                model_path,
                device=f"cuda:{self.local_rank}",
                trust_remote_code=True
            )
            
            # è·å–tokenizer
            if hasattr(model, 'tokenizer'):
                tokenizer = model.tokenizer
            else:
                # æ‰‹åŠ¨åŠ è½½tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_path, 
                    local_files_only=True,
                    trust_remote_code=True
                )
            
            # è®¾ç½®embeddingç»´åº¦
            actual_dim = model.get_sentence_embedding_dimension()
            target_dim = min(self.args.embedding_dim, actual_dim)
            model.embedding_dim = target_dim
            
            print(f"Model loaded, embedding dim: {target_dim} (actual: {actual_dim})")
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else "[PAD]"
            
            print(f"Tokenizer loaded, vocab size: {tokenizer.vocab_size}")
            
            return model, tokenizer
            
        except Exception as e:
            print(f"Error loading model/tokenizer: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def _init_memmap_files(self):
        """åˆå§‹åŒ–å†…å­˜æ˜ å°„æ–‡ä»¶"""
        # ä¼°ç®—æœ€å¤§æ ·æœ¬æ•°ï¼ˆæ¯æ–‡æ¡£çº¦2-4ä¸ªchunkï¼‰
        max_samples = (self.end_idx - self.start_idx) * 4
        
        print(f"Rank {self.rank}: Initializing memmap files for up to {max_samples:,} samples")
        
        # tokenså†…å­˜æ˜ å°„æ–‡ä»¶
        self.tokens_file = self.output_dir / f"{self.base_name}_tokens.dat"
        self.tokens_shape = (max_samples, 2, self.args.max_length)
        self.tokens_mmap = np.memmap(
            self.tokens_file,
            dtype=np.int32,
            mode='w+',
            shape=self.tokens_shape
        )
        
        # latentså†…å­˜æ˜ å°„æ–‡ä»¶
        self.latents_file = self.output_dir / f"{self.base_name}_latents.dat"
        self.latents_shape = (max_samples, self.args.embedding_dim)
        self.latents_mmap = np.memmap(
            self.latents_file,
            dtype=np.float32,
            mode='w+',
            shape=self.latents_shape
        )
        
        # æ–‡æœ¬æ–‡ä»¶ï¼ˆå•ç‹¬å­˜å‚¨ï¼‰
        self.text_file = self.output_dir / f"{self.base_name}_texts.txt"
        self.text_fp = open(self.text_file, 'w', encoding='utf-8')
        
        # ç´¢å¼•æ–‡ä»¶
        self.index_file = self.output_dir / f"{self.base_name}_index.npy"
        
        # å½“å‰å†™å…¥ä½ç½®
        self.current_idx = 0
        self.text_indices = []
    
    def process(self):
        """ä¸»å¤„ç†æµç¨‹"""
        print(f"Rank {self.rank}: Processing {self.end_idx - self.start_idx} documents...")
        
        # å¤„ç†æ–‡æ¡£
        pbar = tqdm(range(self.start_idx, self.end_idx), 
                   desc=f"Rank {self.rank} processing",
                   position=self.rank,
                   leave=False)
        
        for doc_idx in pbar:
            if doc_idx >= len(self.dataset):
                break
            
            try:
                item = self.dataset[doc_idx]
                text = item.get('text', '').strip()
                
                if not text or len(text) < self.args.min_chars:
                    continue
                
                # åˆ†å‰²æ–‡æœ¬
                chunks = self._split_text(text)
                
                for chunk in chunks:
                    # Tokenize
                    tokenized = self.tokenizer(
                        chunk,
                        truncation=True,
                        padding='max_length',
                        max_length=self.args.max_length,
                        return_tensors='np'
                    )
                    
                    # ä¿å­˜tokens
                    if self.current_idx >= self.tokens_shape[0]:
                        # æ‰©å±•å†…å­˜æ˜ å°„æ–‡ä»¶
                        self._expand_memmap()
                    
                    self.tokens_mmap[self.current_idx, 0, :] = tokenized['input_ids'].astype(np.int32)[0]
                    self.tokens_mmap[self.current_idx, 1, :] = tokenized['attention_mask'].astype(np.uint8)[0]
                    
                    # ä¿å­˜æ–‡æœ¬å’Œç´¢å¼•
                    self.text_fp.write(chunk + "\n")
                    self.text_indices.append(self.current_idx)
                    
                    self.current_idx += 1
                    
                    # æ‰¹é‡å¤„ç†embedding
                    if len(self.text_indices) >= self.args.batch_size:
                        self._process_batch()
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({'samples': self.current_idx})
                    
                    # å®šæœŸåˆ·æ–°
                    if self.current_idx % 10000 == 0:
                        self.tokens_mmap.flush()
                        self.text_fp.flush()
                        gc.collect()
                        torch.cuda.empty_cache()
                        
            except Exception as e:
                print(f"\nRank {self.rank}: Error processing document {doc_idx}: {e}")
                continue
        
        pbar.close()
        
        # å¤„ç†æœ€åä¸€æ‰¹
        if self.text_indices:
            self._process_batch()
        
        # ç”Ÿæˆembedding
        print(f"\nRank {self.rank}: Generating embeddings for {self.current_idx} samples...")
        self._generate_embeddings()
        
        # ä¿å­˜ç´¢å¼•
        self._save_index()
        
        # å…³é—­æ–‡ä»¶
        self.text_fp.close()
        del self.tokens_mmap
        del self.latents_mmap
        
        print(f"\nâœ… Rank {self.rank}: Saved {self.current_idx} samples")
        
        # æ”¶é›†æ‰€æœ‰rankçš„ä¿¡æ¯
        if self.world_size > 1:
            total_counts = [0] * self.world_size
            dist.gather_object(self.current_idx, total_counts if self.rank == 0 else None, dst=0)
            
            if self.rank == 0:
                total_all = sum(total_counts)
                print(f"\nğŸ‰ All ranks completed! Total samples: {total_all:,}")
                
                # åˆ›å»ºå…¨å±€ç´¢å¼•
                self._create_global_index(total_counts)
        
        return self.current_idx
    
    def _expand_memmap(self):
        """æ‰©å±•å†…å­˜æ˜ å°„æ–‡ä»¶"""
        print(f"Rank {self.rank}: Expanding memmap files...")
        
        # æ‰©å±•tokens
        new_tokens_shape = (self.tokens_shape[0] * 2, 2, self.args.max_length)
        new_tokens_file = self.output_dir / f"{self.base_name}_tokens_expanded.dat"
        
        # åˆ›å»ºæ–°æ–‡ä»¶å¹¶å¤åˆ¶æ•°æ®
        new_tokens_mmap = np.memmap(
            new_tokens_file,
            dtype=np.int32,
            mode='w+',
            shape=new_tokens_shape
        )
        new_tokens_mmap[:self.tokens_shape[0]] = self.tokens_mmap
        
        # æ›´æ–°å¼•ç”¨
        self.tokens_mmap = new_tokens_mmap
        self.tokens_shape = new_tokens_shape
        
        # æ‰©å±•latents
        new_latents_shape = (self.latents_shape[0] * 2, self.args.embedding_dim)
        new_latents_file = self.output_dir / f"{self.base_name}_latents_expanded.dat"
        
        new_latents_mmap = np.memmap(
            new_latents_file,
            dtype=np.float32,
            mode='w+',
            shape=new_latents_shape
        )
        if hasattr(self, 'latents_written'):
            new_latents_mmap[:self.latents_shape[0]] = self.latents_mmap
        
        self.latents_mmap = new_latents_mmap
        self.latents_shape = new_latents_shape
        
        print(f"Rank {self.rank}: Expanded to {new_tokens_shape[0]:,} samples")
    
    def _process_batch(self):
        """å¤„ç†ä¸€æ‰¹æ–‡æœ¬ç”Ÿæˆembedding"""
        if not self.text_indices:
            return
        
        # è¯»å–æœ€è¿‘çš„æ–‡æœ¬
        start_idx = max(0, self.current_idx - len(self.text_indices))
        text_indices_to_process = self.text_indices.copy()
        self.text_indices = []  # æ¸…ç©ºå½“å‰æ‰¹æ¬¡
        
        # è¯»å–æ–‡æœ¬
        with open(self.text_file, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        
        batch_texts = []
        for idx in text_indices_to_process:
            if idx < len(all_lines):
                batch_texts.append(all_lines[idx].strip())
        
        if not batch_texts:
            return
        
        # ç”Ÿæˆembedding
        with torch.no_grad():
            embeddings = self.model.encode(
                batch_texts,
                convert_to_tensor=True,
                normalize_embeddings=True,
                batch_size=len(batch_texts),
                device=f"cuda:{self.local_rank}",
                show_progress_bar=False
            )
        
        # ä¿å­˜åˆ°å†…å­˜æ˜ å°„
        embeddings_np = embeddings.cpu().numpy().astype(np.float32)
        if embeddings_np.shape[1] > self.args.embedding_dim:
            embeddings_np = embeddings_np[:, :self.args.embedding_dim]
        
        for i, idx in enumerate(text_indices_to_process):
            if idx < self.latents_shape[0]:
                self.latents_mmap[idx, :] = embeddings_np[i]
        
        self.latents_mmap.flush()
        torch.cuda.empty_cache()
    
    def _generate_embeddings(self):
        """ç”Ÿæˆå‰©ä½™çš„æ‰€æœ‰embedding"""
        if not hasattr(self, 'latents_written'):
            self.latents_written = 0
        
        remaining = self.current_idx - self.latents_written
        if remaining <= 0:
            return
        
        print(f"Rank {self.rank}: Generating {remaining} remaining embeddings...")
        
        # è¯»å–æ–‡æœ¬
        with open(self.text_file, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        
        batch_size = self.args.batch_size
        
        pbar = tqdm(range(self.latents_written, self.current_idx, batch_size),
                   desc=f"Rank {self.rank} generating embeddings",
                   position=self.rank + self.world_size,
                   leave=False)
        
        for start_idx in pbar:
            end_idx = min(start_idx + batch_size, self.current_idx)
            batch_indices = list(range(start_idx, end_idx))
            
            batch_texts = []
            for idx in batch_indices:
                if idx < len(all_lines):
                    batch_texts.append(all_lines[idx].strip())
            
            if not batch_texts:
                continue
            
            # ç”Ÿæˆembedding
            with torch.no_grad():
                embeddings = self.model.encode(
                    batch_texts,
                    convert_to_tensor=True,
                    normalize_embeddings=True,
                    batch_size=len(batch_texts),
                    device=f"cuda:{self.local_rank}",
                    show_progress_bar=False
                )
            
            # ä¿å­˜
            embeddings_np = embeddings.cpu().numpy().astype(np.float32)
            if embeddings_np.shape[1] > self.args.embedding_dim:
                embeddings_np = embeddings_np[:, :self.args.embedding_dim]
            
            for i, idx in enumerate(batch_indices):
                if idx < self.latents_shape[0]:
                    self.latents_mmap[idx, :] = embeddings_np[i]
            
            self.latents_written = end_idx
            
            # å®šæœŸåˆ·æ–°
            if start_idx % (batch_size * 100) == 0:
                self.latents_mmap.flush()
                torch.cuda.empty_cache()
        
        pbar.close()
        self.latents_mmap.flush()
    
    def _split_text(self, text):
        """ç®€å•æ–‡æœ¬åˆ†å‰²"""
        if len(text) <= self.args.max_chars:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + self.args.max_chars, len(text))
            
            # å°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²
            chunk = text[start:end]
            if end < len(text):
                split_pos = -1
                for boundary in ['. ', '! ', '? ', '\n\n', '\n']:
                    pos = chunk.rfind(boundary)
                    if pos > len(chunk) * 0.5 and pos > split_pos:
                        split_pos = pos + len(boundary)
                
                if split_pos > 0:
                    chunk = chunk[:split_pos]
                    end = start + split_pos
            
            if len(chunk) >= self.args.min_chars:
                chunks.append(chunk)
            
            start = end
            
            # æ·»åŠ é‡å 
            if self.args.overlap > 0 and start > self.args.overlap:
                start -= self.args.overlap
        
        return chunks
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        index = {
            'total_samples': self.current_idx,
            'tokens_file': str(self.tokens_file),
            'latents_file': str(self.latents_file),
            'text_file': str(self.text_file),
            'tokens_shape': (self.current_idx, 2, self.args.max_length),
            'latents_shape': (self.current_idx, self.args.embedding_dim),
            'embedding_dim': self.args.embedding_dim,
            'max_length': self.args.max_length,
            'rank': self.rank
        }
        
        np.save(self.index_file, index)
    
    def _create_global_index(self, total_counts):
        """åˆ›å»ºå…¨å±€ç´¢å¼•æ–‡ä»¶ï¼ˆåªåœ¨rank 0æ‰§è¡Œï¼‰"""
        index = {
            'total_samples': sum(total_counts),
            'samples_per_rank': total_counts,
            'embedding_dim': self.args.embedding_dim,
            'max_length': self.args.max_length,
            'max_chars': self.args.max_chars,
            'model': self.args.model,
            'model_path': self.args.model_path,
            'world_size': self.world_size,
            'created_at': time.strftime("%Y-%m-%d %H:%M:%S"),
            'source_dir': self.args.source_dir,
            'files': [f"train_rank{r}" for r in range(self.world_size)]
        }
        
        index_path = self.output_dir / "global_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2)
        
        print(f"ğŸ“ Global index saved: {index_path}")

def main():
    parser = argparse.ArgumentParser(description="Memory-Mapped Big File Generator")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--output-dir", required=True, help="Output directory")
    parser.add_argument("--model-path", required=True, help="Local model path")
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--dataset", default="openwebtext", help="Dataset name")
    parser.add_argument("--source-dir", help="Dataset source directory")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model", default="qwen", choices=["qwen", "e5", "sonar"], help="Model type")
    parser.add_argument("--embedding-dim", type=int, default=1024, help="Embedding dimension")
    
    # æ–‡æœ¬å¤„ç†å‚æ•°
    parser.add_argument("--max-length", type=int, default=512, help="Max token length")
    parser.add_argument("--max-chars", type=int, default=4096, help="Max characters per chunk")
    parser.add_argument("--min-chars", type=int, default=50, help="Min characters per chunk")
    parser.add_argument("--overlap", type=int, default=64, help="Overlap between chunks")
    
    # å¤„ç†å‚æ•°
    parser.add_argument("--batch-size", type=int, default=32, help="Processing batch size")
    parser.add_argument("--samples-per-file", type=int, default=200000, help="Target samples per file")
    
    args = parser.parse_args()
    
    # è®¾ç½®åˆ†å¸ƒå¼
    rank, world_size, local_rank = setup_distributed()
    
    try:
        # è¿è¡Œç”Ÿæˆå™¨
        generator = MemmapBigFileGenerator(args, rank, world_size, local_rank)
        total_samples = generator.process()
        
        print(f"\nRank {rank}: Process completed successfully!")
        
    except Exception as e:
        print(f"\nRank {rank}: Error in processing: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        # æ¸…ç†
        if world_size > 1:
            dist.barrier()
            if rank == 0:
                print("\n" + "="*60)
                print("All processes completed!")
                print("="*60)
            dist.destroy_process_group()

if __name__ == "__main__":
    main()